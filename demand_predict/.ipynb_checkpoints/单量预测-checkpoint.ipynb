{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#encoding=utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time,datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.contrib import rnn\n",
    "import pickle\n",
    "\n",
    "#配置matplotlib画图的符号\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  #显示中文\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示坐标中的负号\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#———————————————————数据抽取—————————————————————\n",
    "columns = ['biz_id', 'car_type', 'section', 'week_id', 'holiday_id', 'temperature', 'weather', 'target']\n",
    "from hive_client import HiveClient\n",
    "hive_client = HiveClient()\n",
    "begin_date = '2019-08-17'\n",
    "begin_date_unformatted = '20190817'\n",
    "end_date = '2019-09-04'\n",
    "end_date_unformatted = '20190904'\n",
    "\n",
    "do_matplot = True\n",
    "do_save_model = True\n",
    "model_saved_dir = \"./model_save\"\n",
    "rm_status = os.system('rm -rf '+ model_saved_dir + '/*')\n",
    "\n",
    "load_pickle = True\n",
    "#天气\n",
    "hsql='''\n",
    "    select distinct city_id,temperature,weather_desc,report_time,\n",
    "    from_unixtime(unix_timestamp(report_time,'yyyy-MM-dd HH:mm:ss')+1800,'yyyy-MM-dd HH:mm:ss') as report_end_time\n",
    "    from daojia_ml.express_weather_info\n",
    "    where report_time>='{0}' and report_time<'{1}'\n",
    "'''.format(begin_date, end_date)\n",
    "if(load_pickle == False):\n",
    "    with open('data/weather', 'w+') as f:\n",
    "        status, weather_res = hive_client.query(hsql)\n",
    "        pickle.dump(weather_res, f)\n",
    "else:\n",
    "    with open('data/weather', 'r') as f:\n",
    "        weather_res = pickle.load(f)\n",
    "weather_df = pd.DataFrame(weather_res, columns=['city_id', 'temperature', 'weather', 'start', 'end'])\n",
    "\n",
    "#日期\n",
    "hsql='''\n",
    "    select distinct to_date(service_time) as service_dt,\n",
    "    service_time_week_id, service_time_holiday_id\n",
    "    from daojia_ml.express_odp_order_info_test\n",
    "    where dt>='{0}' and dt<'{1}'\n",
    "'''.format(begin_date_unformatted, end_date_unformatted)\n",
    "if(load_pickle == False):\n",
    "    with open('data/date', 'w+') as f:\n",
    "        status, date_res = hive_client.query(hsql)\n",
    "        pickle.dump(date_res, f)\n",
    "else:\n",
    "    with open('data/date', 'r') as f:\n",
    "        date_res = pickle.load(f)\n",
    "date_df = pd.DataFrame(date_res, columns=['service_dt', 'week_id', 'holiday_id'])\n",
    "\n",
    "\n",
    "#商圈\n",
    "headers='''\n",
    "ADD jar hdfs://nameservice1/bigdata_platform/hive_udf/business-all.jar;\n",
    "CREATE temporary FUNCTION get_biz AS 'com.daojia.business.district.view.commons.validate.BusinessByGps';\n",
    "'''\n",
    "hsql='''\n",
    "    select city_id,car_type,service_time,\n",
    "    split(get_biz(start_gps,cast(city_id as int)),',')[2] as biz_id\n",
    "    from sy_dw_f.f_agt_order_info\n",
    "    where service_time>='{0}' and service_time<'{1}'\n",
    "    and split(get_biz(start_gps,cast(city_id as int)),',')[2]!=''\n",
    "    and need_last_order='Y' and state=7\n",
    "'''.format(begin_date, end_date)\n",
    "if(load_pickle == False):\n",
    "    with open('data/biz', 'w+') as f:\n",
    "        status, biz_res = hive_client.query(hsql, headers=headers)\n",
    "        pickle.dump(biz_res, f)\n",
    "else:\n",
    "    with open('data/biz', 'r') as f:\n",
    "        biz_res = pickle.load(f)\n",
    "biz_df = pd.DataFrame(biz_res, columns=['city_id', 'car_type', 'service_time', 'biz_id'])\n",
    "\n",
    "biz_df = biz_df[(biz_df.biz_id != '') & (biz_df.biz_id!='NULL')]\n",
    "time_section = []\n",
    "time_groupkey = []\n",
    "for index,row in biz_df.iterrows():\n",
    "    t_time = row['service_time'].split(':')[0]\n",
    "    time_section.append(t_time)\n",
    "    time_groupkey.append(row['biz_id']+','+row['car_type']+','+t_time)\n",
    "biz_df['time_section'] = time_section\n",
    "biz_df['time_groupkey'] = time_groupkey\n",
    "time_count = biz_df.groupby(['time_groupkey'])['service_time'].size()\n",
    "\n",
    "\n",
    "# 每小时一个分段\n",
    "cur_time = datetime.datetime.strptime(biz_df['service_time'].min(), '%Y-%m-%d %H:%M:%S')\n",
    "end_time = datetime.datetime.strptime(biz_df['service_time'].max(), '%Y-%m-%d %H:%M:%S')\n",
    "index_uniq = biz_df.groupby(['city_id','biz_id','car_type'])['time_section'].apply(lambda x:len(x.unique()))\n",
    "index_uniq = [idx for idx in index_uniq.index if index_uniq[idx]>=30]\n",
    "vals = []\n",
    "weather_info = {}\n",
    "while cur_time < end_time:\n",
    "    hour = cur_time.hour\n",
    "    cur_time_date = cur_time.strftime('%Y-%m-%d')\n",
    "    cur_time_str = cur_time.strftime('%Y-%m-%d %H')\n",
    "    t_date = date_df[date_df.service_dt==cur_time_date]\n",
    "    t_week = t_date['week_id'].values[0] if len(t_date) > 0 else np.nan\n",
    "    t_holiday = t_date['holiday_id'].values[0] if len(t_date) > 0 else np.nan\n",
    "    for idx in index_uniq:\n",
    "        if cur_time_str not in weather_info:\n",
    "            t_weather = weather_df[(weather_df.city_id==idx[0]) & (weather_df.end>cur_time_str)].sort_values(by=['start'])\n",
    "            t_temperature = t_weather['temperature'].values[0] if len(t_weather)>0 else np.nan\n",
    "            t_weather_val = t_weather['weather'].values[0] if len(t_weather)>0 else np.nan\n",
    "            weather_info[cur_time_str] = (t_temperature, t_weather_val)\n",
    "        t_temperature = weather_info[cur_time_str][0]\n",
    "        t_weather_val = weather_info[cur_time_str][1]\n",
    "        t_biz_key = idx[1] + ',' + idx[2] + ',' + cur_time_str\n",
    "        t_cnt = time_count[t_biz_key] if t_biz_key in time_count.index else 0\n",
    "        vals.append([idx[1], idx[2], cur_time_str, idx[0], t_week, t_holiday, hour, t_temperature, t_weather_val, t_cnt])\n",
    "    cur_time+=datetime.timedelta(hours=1)\n",
    "\n",
    "data = pd.DataFrame(vals, columns=['biz_id','car_type','time_section','city_id','week_id','holiday_id','hour','temperature','weather','cnt']).astype('float32', errors='ignore')\n",
    "print len(data)\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#———————————————————特征处理—————————————————————\n",
    "do_embedding = True\n",
    "do_embedding_update = False\n",
    "embedding_model_path = 'embedding_model/item2vec.model'\n",
    "item_vector_path = 'embedding_model/item_vector'\n",
    "item_vector_dimension = 50\n",
    "\n",
    "if do_embedding:\n",
    "    #index\n",
    "    item_idx_dict = {}\n",
    "    item_dict = {}\n",
    "    item_cnt = 0\n",
    "    item_col = []\n",
    "    for idx,row in data.iterrows():\n",
    "        key = str(row['car_type']) + ',' + str(row['city_id']) + ',' + str(row['week_id']) \\\n",
    "            + ',' + str(row['holiday_id']) + ',' + str(row['hour']) + ',' + str(row['weather']) + ',' + str(row['temperature'])\n",
    "        if key not in item_dict:\n",
    "            item_dict[key] = item_cnt\n",
    "            item_idx_dict[str(item_cnt)] = key\n",
    "            item_cnt += 1\n",
    "        item_col.append(str(item_dict[key]))\n",
    "    item_df = data.loc[:, ['biz_id', 'car_type', 'time_section']]\n",
    "    item_df['item'] = item_col\n",
    "    \n",
    "    #generate sentence\n",
    "    item_list = []\n",
    "    item_groupby = item_df.groupby(['biz_id','car_type'])\n",
    "    for name,group in item_groupby:\n",
    "        t_data = group.sort_values(by=['time_section']).drop(['biz_id','car_type','time_section'], axis=1)\n",
    "        item_list.append(list(t_data['item'].values))\n",
    "    \n",
    "    #embedding\n",
    "    from gensim.models import word2vec\n",
    "    if do_embedding_update:\n",
    "        embedding_model = word2vec.Word2Vec.load(embedding_model_path)\n",
    "        embedding_model.build_vocab(item_list, update=True) \n",
    "        embedding_model.train(item_list, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    else:\n",
    "        embedding_model = word2vec.Word2Vec(item_list, size=item_vector_dimension, sg=1, hs = 0)\n",
    "    embedding_model.save(embedding_model_path)\n",
    "    \n",
    "    from itertools import imap\n",
    "    with open(item_vector_path, 'w+') as f:\n",
    "        for item in embedding_model.wv.index2word:\n",
    "            f.write('%s\\t%s\\n'%(item_idx_dict[item], \",\".join(imap(str,embedding_model[item]))))\n",
    "    \n",
    "    #vector joint\n",
    "    item_vec_cols = [[] for i in range(item_vector_dimension)]\n",
    "    for idx,row in data.iterrows():\n",
    "        key = str(row['car_type']) + ',' + str(row['city_id']) + ',' + str(row['week_id']) \\\n",
    "            + ',' + str(row['holiday_id']) + ',' + str(row['hour']) + ',' + str(row['weather']) + ',' + str(row['temperature'])\n",
    "        item_id = str(item_dict[key])\n",
    "        item_vec = np.zeros(item_vector_dimension)\n",
    "        if item_id in embedding_model:\n",
    "            item_vec = embedding_model[item_id]\n",
    "        for i,t in enumerate(item_vec_cols):\n",
    "            t.append(item_vec[i])\n",
    "    for i in range(item_vector_dimension):\n",
    "        data['embedding_'+str(i)] = item_vec_cols[i]\n",
    "    data = data.drop(['city_id','week_id','holiday_id','hour','weather','temperature'], axis=1)\n",
    "else:\n",
    "    data = data.join(pd.get_dummies(data['car_type'], prefix='car_type', prefix_sep='-'))\n",
    "    data = data.join(pd.get_dummies(data['city_id'], prefix='city_id', prefix_sep='-')).drop(['city_id'], axis=1)\n",
    "    data = data.join(pd.get_dummies(data['week_id'], prefix='week_id', prefix_sep='-')).drop(['week_id'], axis=1)\n",
    "    data = data.join(pd.get_dummies(data['holiday_id'], prefix='holiday_id', prefix_sep='-')).drop(['holiday_id'], axis=1)\n",
    "    data = data.join(pd.get_dummies(data['hour'], prefix='hour', prefix_sep='-')).drop(['hour'], axis=1)\n",
    "    data = data.join(pd.get_dummies(data['weather'], prefix='weather', prefix_sep='-')).drop(['weather'], axis=1)\n",
    "    data = data.join(pd.get_dummies(data['temperature'], prefix='temperature', prefix_sep='-')).drop(['temperature'], axis=1)\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664506 664506\n",
      "598056 598056\n",
      "66450 66450\n"
     ]
    }
   ],
   "source": [
    "#———————————————————形成训练集—————————————————————\n",
    "train_epoch = 1000  # 训练轮数\n",
    "time_step = 30     # 时间步\n",
    "n_layers = 3       # 隐藏层层数\n",
    "rnn_unit = 8        # hidden layer units, 单元数不可大于input_size - 1，一般为log2N\n",
    "batch_size = 128     # 每一批训练多少个样例\n",
    "input_size = len(data.columns)-3     # 输入层数维度\n",
    "output_size = 1      # 输出层数维度\n",
    "original_learning_rate = 0.01         # 学习率\n",
    "data_groupby = data.groupby(['biz_id','car_type'])\n",
    "data_x = []\n",
    "data_y = []\n",
    "data_cnt = 0\n",
    "for name,group in data_groupby:\n",
    "    t_data = group.sort_values(by=['time_section']).drop(['biz_id','car_type','time_section'], axis=1)\n",
    "#     X = t_data.drop(['cnt'], axis=1)\n",
    "    X = t_data\n",
    "    Y = t_data['cnt']\n",
    "    for i in range(time_step, len(X)):\n",
    "        data_x.append(X.iloc[i-time_step:i].astype('float32').values.tolist())\n",
    "        data_y.append(Y.iloc[i])\n",
    "data_x = np.array(data_x)\n",
    "data_y = np.array(data_y)\n",
    "#样本split为10折，9折训练，1折测试\n",
    "train_test_size = 10\n",
    "train_test_indexes = np.array_split(np.random.permutation(len(data_x)), train_test_size)\n",
    "train_x = data_x[train_test_indexes[0]]\n",
    "train_y = data_y[train_test_indexes[0]]\n",
    "for indexes in train_test_indexes[1:train_test_size-1]:\n",
    "    train_x = np.vstack((train_x, data_x[indexes]))\n",
    "    train_y = np.append(train_y, data_y[indexes])\n",
    "test_x = data_x[train_test_indexes[train_test_size-1]]\n",
    "test_y = data_y[train_test_indexes[train_test_size-1]]\n",
    "print len(data_x), len(data_y)\n",
    "print len(train_x),len(train_y)\n",
    "print len(test_x), len(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#———————————————————定义神经网络变量—————————————————————\n",
    "# from tensorflow.python.ops import variable_scope as vs\n",
    "# vs.get_variable_scope().reuse_variables()\n",
    "# None,True,tf.AUTO_REUSE\n",
    "with tf.variable_scope(\"self_variable\", reuse=tf.AUTO_REUSE):\n",
    "    is_training = tf.placeholder(tf.bool, shape=())\n",
    "    X=tf.placeholder(tf.float32, [None,time_step,input_size])    #每批次输入网络的tensor\n",
    "    Y=tf.placeholder(tf.float32, [None,output_size])   # 每批次tensor对应的标签\n",
    "    # 输入层、输出层的权重和偏置\n",
    "    #权重初始化: https://blog.csdn.net/u012328159/article/details/80025785\n",
    "    #xavier: https://tensorflow.google.cn/api_docs/python/tf/contrib/layers/xavier_initializer\n",
    "    #he initialize: https://tensorflow.google.cn/api_docs/python/tf/contrib/layers/variance_scaling_initializer\n",
    "    weights={\n",
    "            'in':tf.get_variable(name='in',shape=[input_size, rnn_unit], initializer=tf.contrib.layers.variance_scaling_initializer(mode='FAN_IN')), #激活函数为relu选择he initialize. He et et al. 2015\n",
    "#             'in':tf.get_variable(name='in',shape=[input_size, rnn_unit], initializer=tf.contrib.layers.xavier_initializer()), #激活函数为sigmod/tanh选择xavier. Glorot et al. 2010\n",
    "    #         'in':tf.Variable(tf.truncated_normal([input_size, rnn_unit], stddev=0.1)), #标准差0.1的正太分布\n",
    "    #         'in':tf.Variable(tf.random_normal([input_size,rnn_unit])), #随机初始化\n",
    "            'out':tf.get_variable(name='out',shape=[rnn_unit, 1], initializer=tf.contrib.layers.variance_scaling_initializer(mode='FAN_OUT'))\n",
    "#             'out':tf.get_variable(name='out',shape=[rnn_unit, 1], initializer=tf.contrib.layers.xavier_initializer()), #激活函数为sigmod/tanh选择xavier. Glorot et al. 2010\n",
    "    #         'out':tf.Variable(tf.random_normal([rnn_unit,1]))\n",
    "    }\n",
    "    biases={\n",
    "        'in':tf.Variable(tf.constant(0.,shape=[rnn_unit,])), #置零\n",
    "        'out':tf.Variable(tf.constant(0.,shape=[1,]))\n",
    "    }\n",
    "    #处理过拟合问题。该值在其起作用的层上，给该层每一个神经元添加一个“开关”，“开关”打开的概率是keep_prob定义的值，一旦开关被关了，这个神经元的输出将被“阻断”。这样做可以平衡各个神经元起作用的重要性，杜绝某一个神经元“一家独大”，各种大佬都证明这种方法可以有效减弱过拟合的风险。\n",
    "    input_keep_prob = tf.placeholder(tf.float32)\n",
    "    output_keep_prob = tf.placeholder(tf.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择记忆细胞\n",
    "def cell_selected(cell, input_rnn):\n",
    "    if cell == \"RNN\":\n",
    "        # 指定激活函数为ReLU函数，然后构造三个RNN细胞状态\n",
    "        # 构建堆叠的RNN模型\n",
    "        # 每个时刻都有一个输出和一个隐状态（或多个隐状态），我们只取最后一个输出和隐状态\n",
    "        # 但是TensofFlow中不知道为啥取了最后时刻的三个隐状态，用于计算最终输出。        \n",
    "        rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=rnn_unit,activation=tf.nn.relu)\n",
    "        if is_training == True:\n",
    "            rnn_cell = tf.contrib.rnn.DropoutWrapper(rnn_cell, input_keep_prob=input_keep_prob, output_keep_prob=output_keep_prob)\n",
    "        multi_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell for layer in range(n_layers)])\n",
    "        outputs, states = tf.nn.dynamic_rnn(multi_cell, input_rnn, dtype=tf.float32)\n",
    "        return tf.concat(axis=1, values=states)\n",
    "        \n",
    "    elif cell == \"LSTM\":\n",
    "        # 构造三个LSTM记忆细胞,不用管激活函数\n",
    "        # states[-1]中包含了长期状态和短期状态，这里取最后一个循环层的短期状态\n",
    "#         lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=rnn_unit, activation=tf.nn.relu)\n",
    "        if is_training == True:\n",
    "            lstm_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units=rnn_unit, activation=tf.nn.relu, layer_norm = True)\n",
    "        else:\n",
    "            lstm_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units=rnn_unit, activation=tf.nn.relu, layer_norm = False)\n",
    "        if is_training == True:\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, input_keep_prob=input_keep_prob)\n",
    "#         lstm_cell = tf.layers.batch_normalization(lstm_cell, axis=0, training=is_training)\n",
    "#         lstm_cell = tf.contrib.layers.batch_norm(lstm_cell, center=True, scale=True, is_training=is_training)\n",
    "        multi_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell for layer in range(n_layers)], state_is_tuple = True)\n",
    "        if is_training == True:\n",
    "            multi_cell = tf.contrib.rnn.DropoutWrapper(multi_cell, output_keep_prob=output_keep_prob)\n",
    "        outputs, states = tf.nn.dynamic_rnn(multi_cell, input_rnn, dtype=tf.float32)\n",
    "#         init_state=multi_cell.zero_state(len(rnn), dtype=tf.float32)   #为避免不同batch-size输入在state初始化时报错，改用默认方式\n",
    "#         outputs,states=tf.nn.dynamic_rnn(multi_cell, input_rnn, initial_state=init_state, time_major=False, dtype=tf.float32)\n",
    "        return states[-1][1]\n",
    "        \n",
    "    elif cell == \"GRU\":\n",
    "        # GRU和LSTM大致相同，但是states[-1]中只包含了短期状态。\n",
    "        gru_cell = tf.nn.rnn_cell.GRUCell(num_units=rnn_unit, activation=tf.nn.relu)\n",
    "        if is_training == True:\n",
    "            gru_cell = tf.contrib.rnn.DropoutWrapper(gru_cell, input_keep_prob=input_keep_prob, output_keep_prob=output_keep_prob)\n",
    "        multi_cell = tf.nn.rnn_cell.MultiRNNCell([gru_cell for layer in range(n_layers)])\n",
    "        outputs, states = tf.nn.dynamic_rnn(multi_cell, input_rnn, dtype=tf.float32)\n",
    "        return states[-1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#———————————————————定义lstm网络—————————————————————\n",
    "def network(cell_type):      #参数：输入网络批次数目\n",
    "    w_in=weights['in']\n",
    "    b_in=biases['in']\n",
    "    input=tf.reshape(X,[-1,input_size])  #需要将tensor转为2维进行计算，计算后的结果作为 隐藏层的输入\n",
    "    #输入层\n",
    "    #激活函数 https://blog.csdn.net/sinat_29957455/article/details/81841278\n",
    "#     input_rnn = tf.nn.relu_layer(input, w_in, b_in)  #差别不大\n",
    "    input_rnn=tf.matmul(input,w_in)+b_in\n",
    "    input_rnn=tf.reshape(input_rnn,[-1,time_step,rnn_unit])   #将tensor转为3维，作为 lstm cell的输入\n",
    "    #定义隐层单元\n",
    "    output_rnn = cell_selected(cell_type, input_rnn)\n",
    "    output_rnn = tf.reshape(output_rnn,[-1,rnn_unit])  #作为输出层的输入\n",
    "    #输出层\n",
    "    w_out=weights['out']\n",
    "    b_out=biases['out']\n",
    "#     pred = tf.nn.relu_layer(output_rnn, w_out, b_out)  #弃用,截断导致模型不更新\n",
    "    pred=tf.matmul(output_rnn, w_out) + b_out\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0904 20:20:07.182662 139938725828416 deprecation.py:323] From <ipython-input-6-2fab504b1439>:27: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0904 20:20:07.185009 139938725828416 rnn_cell_impl.py:1642] At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "W0904 20:20:07.187177 139938725828416 deprecation.py:323] From <ipython-input-6-2fab504b1439>:30: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0904 20:20:07.943828 139938725828416 deprecation.py:506] From /home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0904 20:20:08.659456 139938725828416 deprecation.py:323] From /home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0904 20:22:24.315748 139938725828416 deprecation.py:323] From <ipython-input-8-342b0dd01cc4>:65: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of iterations:', 0, ' mse:', 0.98528475, 'test_mse_', 1.0960697)\n",
      "('Number of iterations:', 0, ' loss:', 0.95443153)\n",
      "('Number of iterations:', 1, ' loss:', 1.205819)\n",
      "('Number of iterations:', 2, ' loss:', 1.4911989)\n",
      "('Number of iterations:', 3, ' loss:', 1.651851)\n",
      "('Number of iterations:', 4, ' loss:', 1.5192215)\n",
      "('Number of iterations:', 5, ' loss:', 1.5188208)\n",
      "('Number of iterations:', 6, ' loss:', 1.5058078)\n",
      "('Number of iterations:', 7, ' loss:', 1.4251248)\n",
      "('Number of iterations:', 8, ' loss:', 1841.4734)\n",
      "('Number of iterations:', 9, ' loss:', 18.499317)\n",
      "('Number of iterations:', 10, ' mse:', 1.8003213, 'test_mse_', 1.9204005)\n",
      "('Number of iterations:', 10, ' loss:', 2.4720368)\n",
      "('Number of iterations:', 11, ' loss:', 1.5680983)\n",
      "('Number of iterations:', 12, ' loss:', 1.7917291)\n",
      "('Number of iterations:', 13, ' loss:', 1.5201912)\n",
      "('Number of iterations:', 14, ' loss:', 1.5287278)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-342b0dd01cc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test mse value \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#对模型进行训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-342b0dd01cc4>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#             test_loop_loss = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_indexes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                                               \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                               \u001b[0minput_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m                                                               \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mloop_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#                 test_loss_=sess.run(loss,feed_dict = {X:test_x, \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[1;32m   1099\u001b[0m                          'graph before calling run().')\n",
      "\u001b[0;32m/home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mversion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3272\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3273\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3275\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/work/anaconda2/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, t, v, tb)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;31m# Internal methods used by condition variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/work/anaconda2/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mrelease\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__owner\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_get_ident\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot release un-acquired lock\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__count\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__owner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#———————————————————对模型进行训练—————————————————————\n",
    "def train_rnn():\n",
    "    global batch_size\n",
    "#     tf.reset_default_graph()\n",
    "    with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred=network(\"LSTM\")\n",
    "    batch_per_epoch = int(np.ceil(len(train_x)*1./batch_size))\n",
    "    #指数型学习率参数\n",
    "    global_step_train = tf.Variable(0, trainable=False) #调用minimize时会通过参数传递自动更新\n",
    "    learning_rate_step = 10*batch_per_epoch #每10轮更新一次梯度\n",
    "    learning_rate_decay = 0.9  #指数更新系数\n",
    "    learning_rate = tf.train.exponential_decay(original_learning_rate, global_step_train, \\\n",
    "                                                        learning_rate_step, learning_rate_decay, staircase=True) #staircase=True表示每decay_steps计算学习速率变化\n",
    "    #定义损失函数\n",
    "    loss = tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  #使用batch normalize\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step_train)\n",
    "    #mse\n",
    "    mse = tf.losses.mean_squared_error(pred, Y)\n",
    "    \n",
    "\n",
    "#     saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        lr_list = [] #收集学习率\n",
    "        loss_list = [] #收集损失值\n",
    "        mse_list = [] #收集mse\n",
    "        test_loss_list = [] #test loss\n",
    "        test_mse_list = [] #test mse\n",
    "        #do early termination\n",
    "        best_test_mse = 1e8\n",
    "        last_improved = 20\n",
    "        max_improve_wait_step = 100\n",
    "        hit_early_termination = False\n",
    "        \n",
    "        for i in range(train_epoch): #模型训练的次数，We can increase the number of iterations to gain better result.\n",
    "            #shuffle the train_data\n",
    "            batch_indexes = np.array_split(np.random.permutation(len(train_x)), batch_per_epoch)\n",
    "            loop_loss = []\n",
    "#             test_loop_loss = []\n",
    "            for batch_index in batch_indexes:\n",
    "                _,loss_=sess.run([train_op,loss],feed_dict = {X:train_x[batch_index], \\\n",
    "                                                              Y:np.reshape(train_y[batch_index], (-1, 1)), \\\n",
    "                                                              input_keep_prob:0.5, output_keep_prob:0.5, \\\n",
    "                                                              is_training:True})\n",
    "                loop_loss.append(loss_)\n",
    "#                 test_loss_=sess.run(loss,feed_dict = {X:test_x, \\\n",
    "#                                                               Y:np.reshape(test_y, (-1, 1)), \\\n",
    "#                                                               input_keep_prob:1., output_keep_prob:1., \\\n",
    "#                                                               is_training:False})\n",
    "#                 test_loop_loss.append(test_loss_)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                mse_ = sess.run(mse, feed_dict = {X:train_x, \\\n",
    "                                                  Y:np.reshape(train_y, (-1, 1)), \\\n",
    "                                                  input_keep_prob:1., output_keep_prob:1., \\\n",
    "                                                  is_training:False})\n",
    "                mse_list.append(mse_)\n",
    "                test_mse_ = sess.run(mse, feed_dict = {X:test_x, \\\n",
    "                                                       Y:np.reshape(test_y, (-1, 1)), \\\n",
    "                                                       input_keep_prob:1., output_keep_prob:1.,\\\n",
    "                                                       is_training:False})\n",
    "                test_mse_list.append(test_mse_)\n",
    "                print(\"Number of iterations:\",i,\" mse:\", mse_, \"test_mse_\", test_mse_) #输出训练次数，输出mse\n",
    "                \n",
    "                if test_mse_ < best_test_mse:\n",
    "                    best_test_mse = test_mse_\n",
    "                    last_improved = i\n",
    "                    \n",
    "                    #model save\n",
    "                    if do_save_model:\n",
    "                        export_path = os.path.join(tf.compat.as_bytes(model_saved_dir), tf.compat.as_bytes(str(i)))\n",
    "                        builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "                        tensor_info_input_keep_prob = tf.saved_model.utils.build_tensor_info(input_keep_prob)\n",
    "                        tensor_info_output_keep_prob = tf.saved_model.utils.build_tensor_info(output_keep_prob)\n",
    "                        tensor_info_training_flag = tf.saved_model.utils.build_tensor_info(is_training)\n",
    "                        tensor_info_x = tf.saved_model.utils.build_tensor_info(X)\n",
    "                        tensor_info_y = tf.saved_model.utils.build_tensor_info(pred)\n",
    "                        prediction_signature = (\n",
    "                            tf.saved_model.signature_def_utils.build_signature_def(\n",
    "                                inputs={'fea': tensor_info_x, \n",
    "                                        'input_keep_prob':tensor_info_input_keep_prob, \n",
    "                                        'output_keep_prob':tensor_info_output_keep_prob, \n",
    "                                        'is_training':tensor_info_training_flag},\n",
    "                                outputs={'pred': tensor_info_y},\n",
    "                                method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n",
    "                        builder.add_meta_graph_and_variables(\n",
    "                            sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                            signature_def_map={\n",
    "                                'predict_data': prediction_signature})\n",
    "                        builder.save()\n",
    "#                     print(\"model_save\",saver.save(sess,'model_save/modle.ckpt')) #第二个参数是保存的地址，可以修改为自己本地的保存地址\n",
    "                \n",
    "                if i - last_improved > max_improve_wait_step:\n",
    "                    hit_early_termination = True\n",
    "                    break\n",
    "                    \n",
    "            loss_list.append(np.mean(loop_loss))\n",
    "#             test_loss_list.append(np.mean(test_loop_loss))\n",
    "            lr_list.append(sess.run(learning_rate))\n",
    "            \n",
    "            print(\"Number of iterations:\",i,\" loss:\", np.mean(loop_loss)) #输出训练次数，输出损失值\n",
    "            \n",
    "            if hit_early_termination:\n",
    "                break\n",
    "                \n",
    "        print(\"The train has finished, train epoch = \", i, \",hit_early_termination=\", hit_early_termination, \"best_test_mse=\", best_test_mse)\n",
    "        \n",
    "        if do_matplot:\n",
    "            plt.plot(np.arange(0,len(lr_list)),lr_list,'+-',color = 'g')\n",
    "            plt.title('learning rate')\n",
    "            plt.ylabel('lr')\n",
    "            plt.show()\n",
    "#             loss_list = [v for v in loss_list if v<5]\n",
    "            plt.plot(np.arange(0,len(loss_list)),loss_list,'+-',color = 'g')\n",
    "            plt.title('Loss trend')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.show()\n",
    "# #             loss_list = [v for v in test_loss_list if v<5]\n",
    "#             plt.plot(np.arange(0,len(test_loss_list)),test_loss_list,'+-',color = 'g')\n",
    "#             plt.title('Loss trend test')\n",
    "#             plt.ylabel('test_loss')\n",
    "#             plt.show()\n",
    "            plt.plot(np.arange(0,len(mse_list)),mse_list,'+-',color = 'g')\n",
    "            plt.title('MSE')\n",
    "            plt.ylabel('mse')\n",
    "            plt.show()\n",
    "            plt.plot(np.arange(0,len(mse_list)),mse_list,'+-',color = 'g')\n",
    "            plt.title('MSE TEST')\n",
    "            plt.ylabel('mse_test')\n",
    "            plt.show()\n",
    "\n",
    "        \n",
    "        #使用最好的模型在测试集上预测mse\n",
    "        if do_save_model:\n",
    "            import_path = os.path.join(tf.compat.as_bytes(model_saved_dir), tf.compat.as_bytes(str(last_improved)))\n",
    "            meta_graph_def = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], import_path)\n",
    "            # 从meta_graph_def中取出SignatureDef对象\n",
    "            signature = meta_graph_def.signature_def\n",
    "            signature_key = \"predict_data\"\n",
    "            fea_key = sess.graph.get_tensor_by_name(signature[signature_key].inputs[\"fea\"].name)\n",
    "            input_keep_prob_key = sess.graph.get_tensor_by_name(signature[signature_key].inputs[\"input_keep_prob\"].name)\n",
    "            output_keep_prob_key = sess.graph.get_tensor_by_name(signature[signature_key].inputs[\"output_keep_prob\"].name)\n",
    "            training_flag_key = sess.graph.get_tensor_by_name(signature[signature_key].inputs[\"is_training\"].name)\n",
    "            pred_key = sess.graph.get_tensor_by_name(signature[signature_key].outputs[\"pred\"].name)\n",
    "            pred_res = sess.run(pred_key, feed_dict={fea_key:test_x, \\\n",
    "                                                     input_keep_prob_key:1., output_keep_prob_key:1., \\\n",
    "                                                     training_flag_key:False})\n",
    "            \n",
    "            test_mse = sess.run(tf.losses.mean_squared_error(pred_res, np.reshape(test_y, (-1, 1))))\n",
    "        else:\n",
    "            test_mse = sess.run(mse, feed_dict = {X:test_x, \\\n",
    "                                        Y:np.reshape(test_y, (-1, 1)), \\\n",
    "                                        input_keep_prob:1., output_keep_prob:1., \\\n",
    "                                        is_training: False})\n",
    "        print(\"test mse value \", test_mse)\n",
    "        \n",
    "train_rnn() #对模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #learning rate = 0.0005, learning_rate_decay=0.9\n",
    "# ('Number of iterations:', 0, ' mse:', 277.3063, 'test_mse_', 300.56897)\n",
    "# ('Number of iterations:', 0, ' loss:', 12636.504)\n",
    "# ('Number of iterations:', 1, ' loss:', 3090.8582)\n",
    "# ('Number of iterations:', 2, ' loss:', 139.15874)\n",
    "# ('Number of iterations:', 3, ' loss:', 64.151405)\n",
    "# ('Number of iterations:', 4, ' loss:', 358.9761)\n",
    "# ('Number of iterations:', 5, ' loss:', 24.930973)\n",
    "# ('Number of iterations:', 6, ' loss:', 9.706715)\n",
    "# ('Number of iterations:', 7, ' loss:', 3.7594965)\n",
    "# ('Number of iterations:', 8, ' loss:', 2.9373744)\n",
    "# ('Number of iterations:', 9, ' loss:', 2.5696945)\n",
    "# ('Number of iterations:', 10, ' mse:', 2.3313763, 'test_mse_', 2.6493483)\n",
    "# ('Number of iterations:', 10, ' loss:', 2.40858)\n",
    "# ('Number of iterations:', 11, ' loss:', 2.279276)\n",
    "# ('Number of iterations:', 12, ' loss:', 2.1252275)\n",
    "# ('Number of iterations:', 13, ' loss:', 1.9203985)\n",
    "# ('Number of iterations:', 14, ' loss:', 1.8085206)\n",
    "# ('Number of iterations:', 15, ' loss:', 1.6113774)\n",
    "# ('Number of iterations:', 16, ' loss:', 1.5209328)\n",
    "# ('Number of iterations:', 17, ' loss:', 1.5583917)\n",
    "# ('Number of iterations:', 18, ' loss:', 1.4670149)\n",
    "# ('Number of iterations:', 19, ' loss:', 1.4575567)\n",
    "# ('Number of iterations:', 20, ' mse:', 1.3990803, 'test_mse_', 1.5741584)\n",
    "# ('Number of iterations:', 20, ' loss:', 1.4843664)\n",
    "# ('Number of iterations:', 21, ' loss:', 1.4182106)\n",
    "# ('Number of iterations:', 22, ' loss:', 1.3836938)\n",
    "# ('Number of iterations:', 23, ' loss:', 1.4110873)\n",
    "# ('Number of iterations:', 24, ' loss:', 1.3238561)\n",
    "# ('Number of iterations:', 25, ' loss:', 1.2520941)\n",
    "# ('Number of iterations:', 26, ' loss:', 1.329352)\n",
    "# ('Number of iterations:', 27, ' loss:', 1.1980892)\n",
    "# ('Number of iterations:', 28, ' loss:', 1.0536932)\n",
    "# ('Number of iterations:', 29, ' loss:', 1.0277358)\n",
    "# ('Number of iterations:', 30, ' mse:', 0.9392812, 'test_mse_', 1.0586816)\n",
    "# ('Number of iterations:', 30, ' loss:', 0.97684735)\n",
    "# ('Number of iterations:', 31, ' loss:', 0.94775194)\n",
    "# ('Number of iterations:', 32, ' loss:', 0.98330265)\n",
    "# ('Number of iterations:', 33, ' loss:', 0.90434134)\n",
    "# ('Number of iterations:', 34, ' loss:', 0.8707473)\n",
    "# ('Number of iterations:', 35, ' loss:', 0.8518898)\n",
    "# ('Number of iterations:', 36, ' loss:', 0.85643715)\n",
    "# ('Number of iterations:', 37, ' loss:', 0.84937006)\n",
    "# ('Number of iterations:', 38, ' loss:', 0.8266152)\n",
    "# ('Number of iterations:', 39, ' loss:', 0.8171346)\n",
    "# ('Number of iterations:', 40, ' mse:', 0.76445526, 'test_mse_', 0.8698976)\n",
    "# ('Number of iterations:', 40, ' loss:', 0.8224517)\n",
    "# ('Number of iterations:', 41, ' loss:', 0.81172)\n",
    "# ('Number of iterations:', 42, ' loss:', 0.7809564)\n",
    "# ('Number of iterations:', 43, ' loss:', 0.7888052)\n",
    "# ('Number of iterations:', 44, ' loss:', 0.79022497)\n",
    "# ('Number of iterations:', 45, ' loss:', 0.77060205)\n",
    "# ('Number of iterations:', 46, ' loss:', 0.76950175)\n",
    "# ('Number of iterations:', 47, ' loss:', 0.7849097)\n",
    "# ('Number of iterations:', 48, ' loss:', 0.7809587)\n",
    "# ('Number of iterations:', 49, ' loss:', 0.7676193)\n",
    "# ('Number of iterations:', 50, ' mse:', 0.7403139, 'test_mse_', 0.8734424)\n",
    "# ('Number of iterations:', 50, ' loss:', 0.7549475)\n",
    "# ('Number of iterations:', 51, ' loss:', 0.7517656)\n",
    "# ('Number of iterations:', 52, ' loss:', 0.7538919)\n",
    "# ('Number of iterations:', 53, ' loss:', 0.75118667)\n",
    "# ('Number of iterations:', 54, ' loss:', 0.7550804)\n",
    "# ('Number of iterations:', 55, ' loss:', 0.7433465)\n",
    "# ('Number of iterations:', 56, ' loss:', 0.7510279)\n",
    "# ('Number of iterations:', 57, ' loss:', 0.75038195)\n",
    "# ('Number of iterations:', 58, ' loss:', 0.7408243)\n",
    "# ('Number of iterations:', 59, ' loss:', 0.7342166)\n",
    "# ('Number of iterations:', 60, ' mse:', 0.7082391, 'test_mse_', 0.80120635)\n",
    "# ('Number of iterations:', 60, ' loss:', 0.73564357)\n",
    "# ('Number of iterations:', 61, ' loss:', 0.73085237)\n",
    "# ('Number of iterations:', 62, ' loss:', 0.73153675)\n",
    "# ('Number of iterations:', 63, ' loss:', 0.7363246)\n",
    "# ('Number of iterations:', 64, ' loss:', 0.7304109)\n",
    "# ('Number of iterations:', 65, ' loss:', 0.7258571)\n",
    "# ('Number of iterations:', 66, ' loss:', 0.7223647)\n",
    "# ('Number of iterations:', 67, ' loss:', 0.7173189)\n",
    "# ('Number of iterations:', 68, ' loss:', 0.720853)\n",
    "# ('Number of iterations:', 69, ' loss:', 0.72392577)\n",
    "# ('Number of iterations:', 70, ' mse:', 0.72717744, 'test_mse_', 0.78538805)\n",
    "# ('Number of iterations:', 70, ' loss:', 0.71902335)\n",
    "# ('Number of iterations:', 71, ' loss:', 0.7130223)\n",
    "# ('Number of iterations:', 72, ' loss:', 0.71265113)\n",
    "# ('Number of iterations:', 73, ' loss:', 0.7117342)\n",
    "# ('Number of iterations:', 74, ' loss:', 0.71052945)\n",
    "# ('Number of iterations:', 75, ' loss:', 0.7063599)\n",
    "# ('Number of iterations:', 76, ' loss:', 0.70881504)\n",
    "# ('Number of iterations:', 77, ' loss:', 0.7062867)\n",
    "# ('Number of iterations:', 78, ' loss:', 0.7141351)\n",
    "# ('Number of iterations:', 79, ' loss:', 0.7118051)\n",
    "# ('Number of iterations:', 80, ' mse:', 0.6848366, 'test_mse_', 0.7991062)\n",
    "# ('Number of iterations:', 80, ' loss:', 0.70438004)\n",
    "# ('Number of iterations:', 81, ' loss:', 0.70149636)\n",
    "# ('Number of iterations:', 82, ' loss:', 0.7020492)\n",
    "# ('Number of iterations:', 83, ' loss:', 0.7036402)\n",
    "# ('Number of iterations:', 84, ' loss:', 0.70198625)\n",
    "# ('Number of iterations:', 85, ' loss:', 0.69468087)\n",
    "# ('Number of iterations:', 86, ' loss:', 0.6996585)\n",
    "# ('Number of iterations:', 87, ' loss:', 0.6944026)\n",
    "# ('Number of iterations:', 88, ' loss:', 0.68889683)\n",
    "# ('Number of iterations:', 89, ' loss:', 0.6899125)\n",
    "# ('Number of iterations:', 90, ' mse:', 0.690834, 'test_mse_', 0.81951153)\n",
    "# ('Number of iterations:', 90, ' loss:', 0.69712675)\n",
    "# ('Number of iterations:', 91, ' loss:', 0.68989813)\n",
    "# ('Number of iterations:', 92, ' loss:', 0.6878052)\n",
    "# ('Number of iterations:', 93, ' loss:', 0.68881375)\n",
    "# ('Number of iterations:', 94, ' loss:', 0.6920298)\n",
    "# ('Number of iterations:', 95, ' loss:', 0.68462986)\n",
    "# ('Number of iterations:', 96, ' loss:', 0.6833937)\n",
    "# ('Number of iterations:', 97, ' loss:', 0.6880767)\n",
    "# ('Number of iterations:', 98, ' loss:', 0.68908626)\n",
    "# ('Number of iterations:', 99, ' loss:', 0.6839101)\n",
    "# ('The train has finished, train epoch = ', 99, ',hit_early_termination=', False, 'best_test_mse=', 0.78538805)\n",
    "# /home/work/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1328: UserWarning: findfont: Font family [u'sans-serif'] not found. Falling back to DejaVu Sans\n",
    "#   (prop.get_family(), self.defaultFamily[fontext]))\n",
    "\n",
    "\n",
    "\n",
    "# W0830 21:57:16.550570 140635288450880 deprecation.py:323] From <ipython-input-7-91c949df90cd>:109: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
    "# Instructions for updating:\n",
    "# This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
    "# W0830 21:57:16.831374 140635288450880 deprecation.py:323] From /home/work/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
    "# Instructions for updating:\n",
    "# Use standard file APIs to check for files with this prefix.\n",
    "# ('test mse value ', 0.78538805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######上海效果\n",
    "#———————————————————数据抽取—————————————————————\n",
    "columns = ['biz_id', 'car_type', 'section', 'week_id', 'holiday_id', 'temperature', 'weather', 'target']\n",
    "from hive_client import HiveClient\n",
    "hive_client = HiveClient()\n",
    "begin_date = '2019-09-03'\n",
    "begin_date_unformatted = '20190903'\n",
    "end_date = '2019-09-04'\n",
    "end_date_unformatted = '20190904'\n",
    "\n",
    "load_pickle = False\n",
    "#天气\n",
    "hsql='''\n",
    "    select distinct city_id,temperature,weather_desc,report_time,\n",
    "    from_unixtime(unix_timestamp(report_time,'yyyy-MM-dd HH:mm:ss')+1800,'yyyy-MM-dd HH:mm:ss') as report_end_time\n",
    "    from daojia_ml.express_weather_info\n",
    "    where report_time>='{0}' and report_time<'{1}'\n",
    "'''.format(begin_date, end_date)\n",
    "if(load_pickle == False):\n",
    "    with open('data/weather_test', 'w+') as f:\n",
    "        status, weather_res = hive_client.query(hsql)\n",
    "        pickle.dump(weather_res, f)\n",
    "else:\n",
    "    with open('data/weather_test', 'r') as f:\n",
    "        weather_res = pickle.load(f)\n",
    "weather_df = pd.DataFrame(weather_res, columns=['city_id', 'temperature', 'weather', 'start', 'end'])\n",
    "\n",
    "#日期\n",
    "hsql='''\n",
    "    select distinct to_date(service_time) as service_dt,\n",
    "    service_time_week_id, service_time_holiday_id\n",
    "    from daojia_ml.express_odp_order_info_test\n",
    "    where dt>='{0}' and dt<'{1}'\n",
    "'''.format(begin_date_unformatted, end_date_unformatted)\n",
    "if(load_pickle == False):\n",
    "    with open('data/date_test', 'w+') as f:\n",
    "        status, date_res = hive_client.query(hsql)\n",
    "        pickle.dump(date_res, f)\n",
    "else:\n",
    "    with open('data/date_test', 'r') as f:\n",
    "        date_res = pickle.load(f)\n",
    "date_df = pd.DataFrame(date_res, columns=['service_dt', 'week_id', 'holiday_id'])\n",
    "\n",
    "\n",
    "#商圈\n",
    "headers='''\n",
    "ADD jar hdfs://nameservice1/bigdata_platform/hive_udf/business-all.jar;\n",
    "CREATE temporary FUNCTION get_biz AS 'com.daojia.business.district.view.commons.validate.BusinessByGps';\n",
    "'''\n",
    "hsql='''\n",
    "    select city_id,car_type,service_time,\n",
    "    split(get_biz(start_gps,cast(city_id as int)),',')[2] as biz_id\n",
    "    from sy_dw_f.f_agt_order_info\n",
    "    where service_time>='{0}' and service_time<'{1}'\n",
    "    and split(get_biz(start_gps,cast(city_id as int)),',')[2]!=''\n",
    "    and need_last_order='Y' and state=7\n",
    "'''.format(begin_date, end_date)\n",
    "if(load_pickle == False):\n",
    "    with open('data/biz_test', 'w+') as f:\n",
    "        status, biz_res = hive_client.query(hsql, headers=headers)\n",
    "        pickle.dump(biz_res, f)\n",
    "else:\n",
    "    with open('data/biz_test', 'r') as f:\n",
    "        biz_res = pickle.load(f)\n",
    "biz_df = pd.DataFrame(biz_res, columns=['city_id', 'car_type', 'service_time', 'biz_id'])\n",
    "\n",
    "biz_df = biz_df[(biz_df.biz_id != '') & (biz_df.biz_id!='NULL')]\n",
    "time_section = []\n",
    "time_groupkey = []\n",
    "for index,row in biz_df.iterrows():\n",
    "    t_time = row['service_time'].split(':')[0]\n",
    "    time_section.append(t_time)\n",
    "    time_groupkey.append(row['biz_id']+','+row['car_type']+','+t_time)\n",
    "biz_df['time_section'] = time_section\n",
    "biz_df['time_groupkey'] = time_groupkey\n",
    "time_count = biz_df.groupby(['time_groupkey'])['service_time'].size()\n",
    "\n",
    "\n",
    "# 每小时一个分段\n",
    "cur_time = datetime.datetime.strptime(biz_df['service_time'].min(), '%Y-%m-%d %H:%M:%S')\n",
    "end_time = datetime.datetime.strptime(biz_df['service_time'].max(), '%Y-%m-%d %H:%M:%S')\n",
    "index_uniq = biz_df.groupby(['city_id','biz_id','car_type'])['time_section'].apply(lambda x:len(x.unique()))\n",
    "# index_uniq = [idx for idx in index_uniq.index if index_uniq[idx]>=30]\n",
    "vals = []\n",
    "weather_info = {}\n",
    "while cur_time < end_time:\n",
    "    hour = cur_time.hour\n",
    "    cur_time_date = cur_time.strftime('%Y-%m-%d')\n",
    "    cur_time_str = cur_time.strftime('%Y-%m-%d %H')\n",
    "    t_date = date_df[date_df.service_dt==cur_time_date]\n",
    "    t_week = t_date['week_id'].values[0] if len(t_date) > 0 else np.nan\n",
    "    t_holiday = t_date['holiday_id'].values[0] if len(t_date) > 0 else np.nan\n",
    "    for idx in index_uniq:\n",
    "        if cur_time_str not in weather_info:\n",
    "            t_weather = weather_df[(weather_df.city_id==idx[0]) & (weather_df.end>cur_time_str)].sort_values(by=['start'])\n",
    "            t_temperature = t_weather['temperature'].values[0] if len(t_weather)>0 else np.nan\n",
    "            t_weather_val = t_weather['weather'].values[0] if len(t_weather)>0 else np.nan\n",
    "            weather_info[cur_time_str] = (t_temperature, t_weather_val)\n",
    "        t_temperature = weather_info[cur_time_str][0]\n",
    "        t_weather_val = weather_info[cur_time_str][1]\n",
    "        t_biz_key = idx[1] + ',' + idx[2] + ',' + cur_time_str\n",
    "        t_cnt = time_count[t_biz_key] if t_biz_key in time_count.index else 0\n",
    "        vals.append([idx[1], idx[2], cur_time_str, idx[0], t_week, t_holiday, hour, t_temperature, t_weather_val, t_cnt])\n",
    "    cur_time+=datetime.timedelta(hours=1)\n",
    "\n",
    "data = pd.DataFrame(vals, columns=['biz_id','car_type','time_section','city_id','week_id','holiday_id','hour','temperature','weather','cnt']).astype('float32', errors='ignore')\n",
    "print len(data)\n",
    "print data.head()\n",
    "\n",
    "#———————————————————特征处理—————————————————————\n",
    "do_embedding = True\n",
    "do_embedding_update = False\n",
    "item_vector_path = 'embedding_model/item_vector'\n",
    "item_vector_dimension = 50\n",
    "\n",
    "if do_embedding:\n",
    "    #index\n",
    "    item_dict = {}\n",
    "    item_cnt = 0\n",
    "    item_col = []\n",
    "    for idx,row in data.iterrows():\n",
    "        key = str(row['car_type']) + ',' + str(row['city_id']) + ',' + str(row['week_id']) \\\n",
    "            + ',' + str(row['holiday_id']) + ',' + str(row['hour']) + ',' + str(row['weather']) + ',' + str(row['temperature'])\n",
    "        if key not in item_dict:\n",
    "            item_dict[key] = item_cnt\n",
    "            item_cnt += 1\n",
    "        item_col.append(str(item_dict[key]))\n",
    "    item_df = data.loc[:, ['biz_id', 'car_type', 'time_section']]\n",
    "    item_df['item'] = item_col\n",
    "    \n",
    "    #generate sentence\n",
    "    item_list = []\n",
    "    item_groupby = item_df.groupby(['biz_id','car_type'])\n",
    "    for name,group in item_groupby:\n",
    "        t_data = group.sort_values(by=['time_section']).drop(['biz_id','car_type','time_section'], axis=1)\n",
    "        item_list.append(list(t_data['item'].values))\n",
    "    \n",
    "    #embedding\n",
    "    from itertools import imap\n",
    "    item_vector = {}\n",
    "    with open(item_vector_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            item,vec_str = line.split('\\t')\n",
    "            weight_vec = [float(w) for w in vec_str.split(',')]\n",
    "            item_vector[item] = weight_vec\n",
    "            \n",
    "    #vector joint\n",
    "    item_vec_cols = [[] for i in range(item_vector_dimension)]\n",
    "    for idx,row in data.iterrows():\n",
    "        key = str(row['car_type']) + ',' + str(row['city_id']) + ',' + str(row['week_id']) \\\n",
    "            + ',' + str(row['holiday_id']) + ',' + str(row['hour']) + ',' + str(row['weather']) + ',' + str(row['temperature'])\n",
    "        item_vec = np.zeros(item_vector_dimension)\n",
    "        if key in item_vector:\n",
    "            item_vec = item_vector[key]\n",
    "        for i,t in enumerate(item_vec_cols):\n",
    "            t.append(item_vec[i])\n",
    "    for i in range(item_vector_dimension):\n",
    "        data['embedding_'+str(i)] = item_vec_cols[i]\n",
    "    data = data.drop(['city_id','week_id','holiday_id','hour','weather','temperature'], axis=1)\n",
    "else:\n",
    "    data = data.join(pd.get_dummies(data['car_type'], prefix='car_type', prefix_sep='-'))\n",
    "    data = data.join(pd.get_dummies(data['city_id'], prefix='city_id', prefix_sep='-')).drop(['city_id'], axis=1)\n",
    "    data = data.join(pd.get_dummies(data['week_id'], prefix='week_id', prefix_sep='-')).drop(['week_id'], axis=1)\n",
    "    data = data.join(pd.get_dummies(data['holiday_id'], prefix='holiday_id', prefix_sep='-')).drop(['holiday_id'], axis=1)\n",
    "    data = data.join(pd.get_dummies(data['hour'], prefix='hour', prefix_sep='-')).drop(['hour'], axis=1)\n",
    "    data = data.join(pd.get_dummies(data['weather'], prefix='weather', prefix_sep='-')).drop(['weather'], axis=1)\n",
    "    data = data.join(pd.get_dummies(data['temperature'], prefix='temperature', prefix_sep='-')).drop(['temperature'], axis=1)\n",
    "print data.head()\n",
    "\n",
    "#———————————————————形成训练集—————————————————————\n",
    "data_groupby = data.groupby(['biz_id','car_type'])\n",
    "data_x = []\n",
    "data_y = []\n",
    "data_cnt = 0\n",
    "for name,group in data_groupby:\n",
    "    t_data = group.sort_values(by=['time_section']).drop(['biz_id','car_type','time_section'], axis=1)\n",
    "#     X = t_data.drop(['cnt'], axis=1)\n",
    "    X = t_data\n",
    "    Y = t_data['cnt']\n",
    "    for i in range(time_step, len(X)):\n",
    "        data_x.append(X.iloc[i-time_step:i].astype('float32').values.tolist())\n",
    "        data_y.append(Y.iloc[i])\n",
    "data_x = np.array(data_x)\n",
    "data_y = np.array(data_y)\n",
    "#样本split为10折，9折训练，1折测试\n",
    "train_test_size = 10\n",
    "train_test_indexes = np.array_split(np.random.permutation(len(data_x)), train_test_size)\n",
    "train_x = data_x[train_test_indexes[0]]\n",
    "train_y = data_y[train_test_indexes[0]]\n",
    "for indexes in train_test_indexes[1:train_test_size-1]:\n",
    "    train_x = np.vstack((train_x, data_x[indexes]))\n",
    "    train_y = np.append(train_y, data_y[indexes])\n",
    "test_x = data_x[train_test_indexes[train_test_size-1]]\n",
    "test_y = data_y[train_test_indexes[train_test_size-1]]\n",
    "print len(data_x), len(data_y)\n",
    "print len(train_x),len(train_y)\n",
    "print len(test_x), len(test_y)\n",
    "\n",
    "\n",
    "#———————————————————样本预测—————————————————————\n",
    "import_path = os.path.join(tf.compat.as_bytes(model_saved_dir), tf.compat.as_bytes(str(last_improved)))\n",
    "meta_graph_def = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], import_path)\n",
    "# 从meta_graph_def中取出SignatureDef对象\n",
    "signature = meta_graph_def.signature_def\n",
    "signature_key = \"predict_data\"\n",
    "fea_key = sess.graph.get_tensor_by_name(signature[signature_key].inputs[\"fea\"].name)\n",
    "input_keep_prob_key = sess.graph.get_tensor_by_name(signature[signature_key].inputs[\"input_keep_prob\"].name)\n",
    "output_keep_prob_key = sess.graph.get_tensor_by_name(signature[signature_key].inputs[\"output_keep_prob\"].name)\n",
    "training_flag_key = sess.graph.get_tensor_by_name(signature[signature_key].inputs[\"is_training\"].name)\n",
    "pred_key = sess.graph.get_tensor_by_name(signature[signature_key].outputs[\"pred\"].name)\n",
    "pred_res = sess.run(pred_key, feed_dict={fea_key:shanghai_test_x, \\\n",
    "                                         input_keep_prob_key:1., output_keep_prob_key:1., \\\n",
    "                                         training_flag_key:False})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
